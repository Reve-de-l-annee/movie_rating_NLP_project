<h3>줄거리를 활용한 영화 관람등급 예측</h3>

- NLP 트랙학습 project

---

<h3>Information</h3>

- 팀원: 이수현, 이채영, 한창헌

**About Project**

1. Introduction


영상물 등급을 평가하는데 오랜 시간이 걸린다.하지만 이 등급은 영화의 수익성과 밀접하게 연관이 되어 있어서 등급이 나온 후 조정을 위해 다시 영화를 찍거나 편집하는 등의 노력이 동반된다. 만약 딥러닝을 활용하여 등급을 예측 할 수 있다면 사전에 많은 조치를 취할 수 있을 것이다. 

프로젝트는 다음과 같은 순서로 진행된다.

- 네이버 영화 사이트로 부터 줄거리와 영상 등급 크롤링 해오기

- koBert 모델에 맞는 데이터 전처리 해주기

- koBert 모델 학습시키기

2.Our Approach

전처리는 영화 줄거리 데이터와 라벨인 등급을 리스트로 매핑하는 것으로 시작된다. 그 후 리스트의 각각의 요소들은 토크나이저를 거쳐 임베딩된다. 이렇게 얻은 토큰화 데이터를 입력값으로 사용하여 사전 학습된 bert에  (768, 4 )크기의 피드 포워드 레이어를 추가해 분류를 수행하였다.

2.1 Data

 네이버 영화 사이트를 크롤링 해서 데이터 프레임에 저장하였다. 10만 개의 데이터를 크롤링 해왔지만 줄거리와 영상물 등급이 모두 있는 영화만을 남기다 보니, 데이터 프레임에는  31743개의 줄거리, 평론가 평점, 네티즌 평점, 제목, 영어 제목, 상영시간, 메이킹 노츠, 영상물 등급 등의 데이터만 남았다. 이 중에서도 다른 컬럼은 쓰지 않고, 우리는 줄거리와, 등급만을 이용한다. 모델에 학습시키기 위해 '청소년 관람불가'를 0으로, '15세 관람가'를 1로, '12세 관람가'를 2, 그리고 '전체 관람가'를 3으로 매핑시켰다.

EDA를 통해, 각각의 관람가에 해당하는 영화 줄거리의 키워드가 두드러지게 다르다는 것을 알게 되었다. 예를 들어서,12세 관람가 영화에는 '마을','마음','세계' 등의 키워드가, 청소년 관람불가 영화에서는 '살인' ,'죽음','섹스' 등의 키워드가 워드 클라우드 결과로 나왔다.

그리고간단한 통계기법을 사용하여 줄거리는 1단어에서 6000단어사이의 길이로 구성되어 있으며, 평균은 384byte 라는 것을 알게되었다. 문장의 길이가 다양하므로 우리는 최대 길이 128로 맞춰 줄 것이다. 또한, 이것은 한 문장안에 동음이의어 등의 문맥을 고려해하는 단어들이 다수 포함되어 있다는 것을 의미한다. 그래서 문맥을 고려한 임베딩 모델을 사용하기로 하였다. BERT는 양방향으로 문장을 읽는 트랜스포머 모델을 기반으로 하여 문맥을 고려하는 모델로 유명하다.

이 프로젝트에서 우리는 sk 텔레콤에서 공개한 오픈소스 모델인 koBERT를 쓴다. 역서에서 설명하는 여러 모델들은 한국어에서 좋은 성능을 보아지 못한다. 이는 한국어 학습 데이터 양이 충분하지 않았기 때문이다. koBERT는 한국어 데이터의 정확도를 높이기 위해 한국어 위키피디아에서 약 500만 개의 문장과 5400만 개의 단어를 학습시켰고, 많은 한국어 다운스트림 테스크에서 좋은 성적을 보였다. 이번 프로젝트를 수행하기 위해 사전학습된 koBERT 모델을 finetuning 하는 방식으로 영상 등급을 판단하는 모델을 만들었다.

한글 문장을 koBERT에 넣기 위해 koBERTTokenizer를 활용하며, 그후 사전학습된 koBERT 모델을 가중치를 우리 데이터 셋에 맞게 조정하는 finetunnig을 수행하여 분류를 수행할 것이다.

2.2 Embedding

기본 BERT에서는 데이터를 입력하기 전에 3가지 임베딩 레이어를 기반으로 입력 데이터를 임베딩으로 변환해야한다.

- 토큰 임베딩 : 문장을 토큰화하여 토큰들을 추출하고, 문장의 시작에는 [CLS]를 끝에는 [SEP]을 추가한다.

- 세그먼트 임베딩 : 각각의 토큰들이 어떤 문장의 소속인지 구별해주는 임베딩이다.

- 위치 임베딩 : 트랜스포머가 모든 단어를 병렬로 처리하므로 단어 순서에 관한 정보를 제공해주는 임베딩이다.

그 후 워드피스 토크나이저를 이용해서 하위 단어로 분할하며 토큰화를 마무리한다.

2.3 Classification Model

우리는 사전 학습된 bert에 (768, 4 )크기의 피드 포워드 레이어를 추가해서 모델을 만들었다.

BERT는 마스크 언어 모델링 즉 문장내 토큰 중 15%를 마스킹하여 이 단어를 예측하도록 각 레이어를 거치면서 가중치를 업데이트 해 나간다. 분류를 위해서는 마지막 레이어를 거친  [CLS] 토큰의 값을 가져와 소프트 맥스 함수를 사용해 각 클래스별 확률 값을 계산하여 분류를 수행한다. [CLS] 토큰을 사용하는 이유는 encoder layer를 거치면서 나머지 token들의 정보를 바탕으로 representation이 생성되며, 이는 곧 나머지 토큰들의 정보를 담고 있다는 의미이다. 이와 같은 이유로 [CLS] token의 output을 classifier에 넘겨주게 되는 것이다.

---

**Flask**

![Directory Structure](https://user-images.githubusercontent.com/74871527/186577990-47223fd1-350d-45e2-a6f7-72939ab539ae.png)
